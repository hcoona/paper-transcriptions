\hypertarget{data-structures-and-algorithms}{%
\chapter{Data Structures and
Algorithms}\label{data-structures-and-algorithms}}

The present section focuses on data structures and algorithms found in
mature data management systems but usually not in college-level text
books; the subsequent sections cover transactional techniques, B-trees
and their usage in database query processing, and B-tree utilities.

While only a single sub-section below is named ``data compression,''
almost all sub-sections pertain to compression in some form: storing
fewer bytes per record, describing multiple records together, comparing
fewer bytes in each search, modifying fewer bytes in each update, and
avoiding fragmentation and wasted space. Efficiency in space and time is
the theme of this section.

The following sub-sections are organized such that the first group
pertains to the size and internal structure of nodes, the next group to
compression specific to B-trees, and the last group to management of
free space. Most of the techniques in the individual sub-sections are
independent of others, although certain combinations may ease their
implementation. For example, prefix- and suffix-truncation require
detailed and perhaps excessive record keeping unless key values are
normalized into binary strings.

\hypertarget{node-size}{%
\section{Node Size}\label{node-size}}

Even the earliest papers on B-trees discussed the optimal node size for
B-trees on disk {[}7{]}. It is governed primarily by access latency and
transfer bandwidth as well as the record size. High latency and high
bandwidth both increase the optimal node size; therefore, the optimal
node size for modern disks approaches 1 MB and the optimal on flash
devices is just a few KB {[}50{]}. A node size with equal access latency
and transfer time is a promising heuristic --- it guarantees a sustained
transfer bandwidth at least half of the theoretical optimum as well as
an I/O rate at least half of the theoretical optimum. It is calculated
by multiplying access latency and transfer bandwidth. For example, for a
disk with 5 ms access latency and 200 MB/s transfer bandwidth, this
leads to 1 MB. An estimated access latency of 0.1 ms and a transfer
bandwidth of 100 MB/s lead to 10 KB as a promising node size for B-trees
on flash devices.

For a more precise optimization, the goal is to maximize the number of
comparisons per unit of I/O time. Examples for this calculation can
already be found in the original B-tree papers {[}7{]}. This
optimization assumes that the goal is to optimize root-to-leaf searches
and not large range scans, I/O time and not CPU effort is the
bottleneck, binary search is used within nodes, and a fixed total number
of comparisons in a root-to-leaf B-tree search independent of the node
size as discussed above.

\autoref{fig-3-1} shows a calculation similar to those in {[}57{]}. It assumes
pages filled to $70\%$ with records of 20 bytes, typical in secondary
indexes. For example, in a page of 4 KB holding 143 records, binary
search performs a little over 7 comparisons on average. The number of
comparisons is termed the utility of the node with respect to searching
the index. I/O times in \autoref{fig-3-1} are calculated assuming 5 ms access
time and 200 MB/s (burst) transfer bandwidth. The heuristic above would
suggest a page size of 5 ms × 200 MB/s = 1,000 KB. B-tree nodes of 128
KB enable the most comparisons (in binary search) relative to the disk
device time. Historically common disk pages of 4 KB are far from optimal
for B-tree indexes on traditional disk drives. Different record sizes
and different devices will result in different optimal page sizes for
B-tree indexes. Most importantly, devices based on flash devices may
achieve 100 times faster access times without substantially different
transfer bandwidth. Optimal B-tree node sizes will be much smaller,
e.g., 2 KB {[}50{]}.

\begin{figure}
  \centering
  \begin{tabular}{rrrrr}
    \centeringcell{Page size} & \centeringcell{Records} & \centeringcell{Node} & \centeringcell{I/O time} & \centeringcell{Utility} \\
    \centeringcell{{[}KB{]}} & \centeringcell{/ page} & \centeringcell{utility} & \centeringcell{{[}ms{]}} & \centeringcell{/ time} \\
    4 & 143 & 7.163 & 5.020 & 1.427 \\
    16 & 573 & 9.163 & 5.080 & 1.804 \\
    64 & 2,294 & 11.163 & 5.320 & 2.098 \\
    128 & 4,588 & 12.163 & 5.640 & 2.157 \\
    256 & 9,175 & 13.163 & 6.280 & 2.096 \\
    1,024 & 36,700 & 15.163 & 10.120 & 1.498 \\
    4,096 & 146,801 & 17.163 & 25.480 & 0.674 \\
  \end{tabular}
  \caption{Utility for pages sizes one a traditional disk.\label{fig-3-1}}
\end{figure}

\begin{itemize}
\item
  The node size should be optimized based on latency and bandwidth of
  the underlying storage. For example, the optimal page size differs for
  traditional disks and semiconductor storage.
\end{itemize}

\hypertarget{interpolation-search}{%
\section{Interpolation Search}\label{interpolation-search}}

\footnote{Much of this section is derived from {[}45{]}.}Like binary
search, interpolation search employs the concept of a remaining search
interval, initially comprising the entire page. Instead of inspecting
the key in the center of the remaining interval like binary search,
interpolation search estimates the position of the sought key value,
typically using a linear interpolation based on the lowest and highest
key value in the remaining interval. For some keys, e.g., artificial
identifier values generated by a sequential process such as invoice
numbers in a business operation, interpolation search works extremely
well.

In the best case, interpolation search is practically unbeatable.
Consider an index on the column Order-Number in the table Orders given
that order numbers and invoice numbers are assigned sequentially. Since
each order number exists precisely once, interpolation among hundreds or
even thousands of records within a B-tree node instantly guides the
search to the correct record.

In the worst case, however, the performance of pure interpolation search
equals that of linear search due to a nonuniform distribution of key
values. The theoretical complexity is $\mathcal{O}(\log\log N)$ for search among
$N$ keys {[}36, 107{]}, or 2 to 4 steps for practical page sizes.
Thus, if the sought key has not yet been found after 3 or 4 steps, the
actual key distribution is not uniform and it might be best to perform
the remaining search using binary search.

Rather than switching from pure interpolation search to pure binary
search, a gradual transition may pay off. If interpolation search has
guided the search to one end of the remaining interval but not directly
to the sought key value, the interval remaining for binary search may be
very small or very large. Thus, it seems advisable to bias the last
interpolation step in such a way to make it very likely that the sought
key is in the smaller remaining interval.

The initial interpolation calculation might use the lowest and highest
possible values in a page, the lowest and highest actual values, or a
regression line based on all current values. The latter technique may be
augmented with a correlation calculation that guides the initial search
steps toward interpolation or binary search. Sums and counts required to
quickly derive regression and correlation coefficients can easily be
maintained incrementally during updates of individual records in a page.

\autoref{fig-3-2} shows two B-tree nodes and their key values. In the upper
one, the correlation between slot numbers and key values is very high
($> 0.998$). Slope and intercept are $3.1$ and $5.9$,
respectively (slot numbers start with 0). An interpolation search for
key value 12 immediately probes slot number $(12 − 5.9) \div
3.1 = 2$ (rounded), which is where key value 12 indeed can be
found. In other words, if the correlation between position and key value
is very strong, interpolation search is promising. In the lower B-tree
node shown in \autoref{fig-3-2}, slope and intercept are −64 and 31,
respectively. More importantly, the correlation coefficient is much
lower ($< 0.75$). Not surprisingly, interpolation search
for key value 97 starts probing at slot $(97 − 64) \div 31 = 5$ whereas the
correct slot number of key value 97 is 8. Thus, if the correlation
between position and key value is weak, binary search is the more
promising approach.

\begin{figure}
  \centering
  \begin{tabular}{l}
    \hline
    \multicolumn{1}{|l|}{5, 9, 12, 16, 19, 21, 25, 27, 31, 34, 36} \\
    \hline\\
    \\
    \hline
    \multicolumn{1}{|l|}{5, 6, 15, 15, 16, 43, 95, 96, 97, 128, 499} \\
    \hline
  \end{tabular}
  \caption{Sample key values.\label{fig-3-2}}
\end{figure}

\begin{itemize}
\item
  If the key value distribution within a page is close to uniform,
  interpolation search requires fewer comparisons and incurs fewer cache
  faults than binary search. Artificial identifiers such as order
  numbers are ideal cases for interpolation search.
\item
  For cases on non-uniform key value distributions, various techniques
  can prevent repeated erroneous interpolation.
\end{itemize}

\hypertarget{variable-length-records}{%
\section{Variable-length Records}\label{variable-length-records}}

While B-trees are usually explained for fixed-length records in the
leaves and fixed-length separator keys in the branch nodes, B-trees in
practically all database systems support variable-length records and
variable-length separator keys. Thus, space management within B-tree
nodes is not trivial.

The standard design for variable-length records in fixed-length pages,
both in B-trees and in heap files, employs an indirection vector (also
known as slot array) with entries of fixed size. Each entry represents
one record. An entry must contain the byte offset of the record and may
contain additional information, e.g., the size of the record.

\autoref{fig-3-3} shows the most important parts of a disk page in a database.
The page header, shown far left within the page, contains index
identifier, B-tree level (for consistency checks), record count, etc.
This is followed in \autoref{fig-3-3} by the indirection vector. In heap files,
slots remain unused after a record deletion in order to ensure that the
remaining valid records retain their record identifier. In B-trees,
insertions or deletions require shifting some slot entries in order to
ensure that binary search can work correctly. (Figure 4.7 in Section 4.2
shows an alternative to this traditional design with less shifting due
to intentional gaps in the sequence of records.) Each used slot contains
a pointer (in form of a byte offset within the page) to a record. In the
diagram, the indirection vector grows from left to right and the set of
records grows from right to left. The opposite design is also possible.
Letting two data structures grow toward each other enables equally well
many small records or fewer large records.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{./media/image1.png}
  \caption{A database page with page header, indirection vector, and
  variable-length records.\label{fig-3-3}}
\end{figure}

For efficient binary search, the entries in the indirection vector are
sorted on their search key. It is not required that the entries be
sorted on their offsets, i.e., the placement of records. For example,
the sequence of slots in the left half of \autoref{fig-3-3} differs from the
sequence of records in the right half. A sort order on offsets is needed
only temporarily for consistency checks and for compaction or free space
consolidation, which may be invoked by a record insertion, by a
size-changing record update, or by a defragmentation utility.

Record insertion requires free space both for the record and for the
entry in the indirection vector. In the standard design, the indirection
vector grows from one end of the page and the data space occupied by
records grows from the opposite end. Free space for the record is
usually found very quickly by growing the data space into the free space
in the middle. Free space for the entry requires finding the correct
placement in the sorted indirection vector and then shifting entries as
appropriate. On average, half of the indirection must shift by one
position.

Record deletion is fast as it typically just leaves a gap in the data
space. However, it must keep the indirection vector dense and sorted,
and thus requires shifting just like insertion. Some recent designs
require less shifting {[}12{]}. Some designs also separate separator
keys and child pointers in branch nodes in order to achieve more
effective compression as well as more efficient search within each
branch node. Those techniques are also discussed below.

\begin{itemize}
\item
  Variable-size records can be supported efficiently by a level of
  indirection within a page.
\item
  Shift operations in the indirection vector can be minimized by gaps
  (invalid entries).
\end{itemize}

\hypertarget{normalized-keys}{%
\section{Normalized Keys}\label{normalized-keys}}

In order to reduce the cost of comparisons, many implementations of
B-trees transform keys into a binary string such that simply binary
comparisons suffice to sort the records during index creation and to
guide a search in the B-tree to the correct record. The key sequence for
the sort order of the original key and for the binary string are the
same, and all comparisons equivalent. This binary string may encode
multiple columns, their sort direction (e.g., descending) and collation
including local characters (e.g., case-insensitive German), string
length or string termination, etc.

Key normalization is a very old technique. It is already mentioned by
Singleton {[}118{]} without citation, presumably because it seemed a
well-known or trivial concept: ``integer comparisons were used to order
normalized floating-point numbers.''

\autoref{fig-3-4} illustrates the idea based on an integer column followed by
two string columns. The initial single bit (shown underlined) indicates
whether the leading key column contains a valid value. Using 0 for null
values and 1 for other values ensures that a null value ``sorts lower''
than all other values. If the integer column value is not null, it is
stored in the next 32 bits. Signed integers require reversing some bits
to ensure the proper sort order, just like floating point values require
proper treatment of exponent, mantissa, and the two sign bits. \autoref{fig-3-4}
assumes that the first column is unsigned. The following single bit
(also shown underlined) indicates whether the first string column
contains a valid value. This value is shown here as text but really
ought to be stored in a binary format as appropriate for the desired
international collation sequence. A string termination symbol (shown as
\textbackslash{}0) marks the end of the string. A termination symbol is
required to ensure the proper sort order. A length indicator, for
example, would destroy the main value of normalized keys, namely sorting
with simple binary comparisons. If the string termination symbol can
occur as a valid character in some strings, the binary representation
must offer one more symbol than the alphabet contains. Notice the
difference in representations between a missing value in a string column
(in the third row) and an empty string (in the fourth row).

\begin{figure}
  \centering
  \begin{tabular}{llll}
Integer & First string & Second string & Normalized key \\
2 & ``flow'' & ``error'' & \underline{1} 0…0 0000 0000 0010 \underline{1} flow\textbackslash{}0 \underline{1} error\textbackslash{}0 \\
3 & ``flower'' & ``rare'' & \underline{1} 0…0 0000 0000 0011 \underline{1} flower\textbackslash{}0 \underline{1} rare\textbackslash{}0 \\
1024 & Null & ``brush'' & \underline{1} 0…0 0100 0000 0000 \underline{0} \underline{1} brush\textbackslash{}0 \\
Null & ``'' & Null & \underline{0} \underline{1} \textbackslash{}0 \underline{0}
  \end{tabular}
  \caption{Normalized keys.\autoref{fig-3-4}}
\end{figure}

For some collation sequences, ``normalized keys'' lose information. A
typical example is a language with lower and upper case letters sorted
and indexed in a case-insensitive order. In that case, two different
original strings might map to the same normalized key, and it is
impossible from the normalized key to decide which original style was
used. One solution for this problem is to store both the normalized key
and the original string value. A second solution is to append to the
normalized key the minimal information that enables a precise recovery
of the original writing style. A third solution, specific to B-tree
indexes, is to employ normalized keys only in branch nodes; recall that
key values in branch nodes merely guide the search to the correct child
but do not contain user data.

In many operating systems, appropriate functions are provided to compute
a normalized key from a localized string value, date value, or time
value. This functionality is used, for example, to list files in a
directory as appropriate for the local language. Adding normalization
for numeric data types is relatively straightforward, as is
concatenation of multiple normalized values. Database code must not rely
on such operating system code, however. The problem with relying on
operating systems support for database indexes is the update frequency.
An operating system might update its normalization code due to an error
or extension in the code or in the definition of a local sort order; it
is unacceptable, however, if such an update silently renders existing
large database indexes incorrect.

Another issue with normalized keys is that they tend to be longer than
the original string values, in particular for some languages and their
complex rules for order, sorting, and index look-up. Compression of
normalized keys seems quite possible but a detailed description seems to
be missing yet in the literature. Thus, normalized keys are currently
used primarily in internal B-tree nodes, where they simplify the
implementation of prefix and suffix truncation but never require
recovery of original key values.

\begin{itemize}
\item
  Normalized keys enable comparisons by traditional hardware
  instructions, much faster than column-by-column interpolation of
  metadata about international sort order, ascending versus descending
  sort order, etc.
\item
  Normalized keys can be longer than a traditional representation but
  are amenable to compression.
\item
  Some systems employ normalized keys in branch nodes but not in leaf
  nodes.
\end{itemize}

\hypertarget{prefix-b-trees}{%
\section{Prefix B-trees}\label{prefix-b-trees}}

Once keys have been normalized into a simple binary string, another
B-tree optimization becomes much easier to implement, namely prefix and
suffix truncation or compression {[}10{]}. Without key normalization,
these techniques would require a fair bit of bookkeeping, even if they
were applied only to entire key fields rather than to individual bytes;
with key normalization, their implementation is relatively
straightforward.

Prefix truncation analyzes the keys in a B-tree node and stores the
common prefix only once, truncating it from all keys stored in the node.
Saving storage space permits increasing the number of records per leaf
and increasing the fan-out of branch nodes. In addition, the truncated
key bytes do not need to be considered in comparisons during a search.

\autoref{fig-3-5} shows the same records within a B-tree node represented
without and with prefix truncation. It is immediately obvious that the
latter representation is more efficient. It is possible to combine
prefix truncation with some additional compression technique, e.g., to
eliminate symbols from the birthdates given. Of course, it is always
required to weigh gains in run-time performance and storage efficiency
against implementation complexity including testing effort.

\begin{figure}
  \centering
  \includegraphics[width=0.6\columnwidth]{./media/fig-3-5.png}

  \caption{A B-tree node without and with prefix truncation.\label{fig-3-5}}
\end{figure}

Prefix truncation can be applied to entire nodes or to some subset of
the keys within a node. Code simplicity argues for truncating the same
prefix from all entries in a B-tree node. Moreover, one can apply prefix
truncation based on the actual keys currently held in a node or based on
the possible key range as defined by the separator keys in parent (and
possibly other ancestor) pages. Code simplicity, in particular for
insertions, argues for prefix truncation based on the maximal possible
key range, even if prefix truncation based on actual keys might produce
better compression {[}87{]}. If prefix truncation is based on actual
keys, insertion of a new key might force reformatting all existing keys.
In an extreme case, a new record might be much smaller than the free
space in a B-tree page yet its insertion might force a page split.

The maximal possible key range for a B-tree page can be captured by
retaining two fence keys in each node, i.e., copies of separator keys
posted in parent nodes while splitting nodes. Figure 4.11 (in Section
4.4) illustrates fence keys in multiple nodes in a B-tree index. Fence
keys have multiple benefits in B-tree implementations, e.g., for key
range locking. With respect to prefix truncation, the leading bytes
shared by the two fence keys of a page define the bytes by all current
and future key values in the page. At the same time, prefix truncation
reduces the overhead imposed by fence keys, and suffix truncation
(applied when leaf nodes are split) ensures that separator keys and thus
fence keys are always as short as possible.

Prefix truncation interacts with interpolation search. In particular, if
the interpolation calculation uses fixed and limited precision,
truncating common prefixes enables more accurate interpolation. Thus,
normalized keys, prefix truncation, suffix truncation, and interpolation
search are a likely combination in an implementation.

A very different approach to prefix truncation technique is offsetvalue
coding {[}28{]}. It is used in high-performance implementations of
sorting, in particular in sorted runs and in the merge logic {[}72{]}.
In this representation, each string is compared to its immediate
predecessor in the sorted sequence and the shared prefix is replaced by
an indication of its length. The sign bit is reserved to make the
indicator order-preserving, i.e., a short shared prefix sorts later than
a long shared prefix. The result is combined with the data at this
offset such that a single machine instruction can compare both offset
and value. This representation saves more space than prefix truncation
applied uniformly to an entire page. It is very suitable to sequential
scans and merging but not to binary search or interpolation search.
Instead, a trie representation could attempt to combine the advantages
of prefix truncation and binary search, but it is used in very few
database systems. The probable reasons are code complexity and update
overhead.

Even if prefix truncation is not implemented in a B-tree and its page
format, it can be exploited for faster comparisons and thus faster
search. The following technique might be called dynamic prefix
truncation. While searching for the correct child pointer in a parent
node, the two keys flanking the child pointer will be inspected. If they
agree on some leading bytes, all keys found by following the child
pointer must agree on the same bytes, which can be skipped in all
subsequent comparisons. It is not necessary to actually compare the two
neighboring separator keys with each other, because the required
information is readily available from the necessary comparisons of these
separator keys with the search key. In other words, dynamic prefix
truncation can be exploited without adding comparison steps to a
root-to-leaf search in a B-tree.

For example, assume a binary search within the B-tree node shown on the
left side of \autoref{fig-3-5}, with the remaining search interval from
``Smith, Jack'' to ``Smith, Jason.'' Thus, the search argument must be
in that range and also start with ``Smith, Ja.'' For all remaining
comparisons, this prefix may be assumed and thus skipped in all
remaining comparisons within this search. Note that dynamic prefix
truncation also applies to B-tree nodes stored with prefix truncation.
In this example, the string ``a'' beyond the truncated prefix ``Smith,
J'' may be skipped in all remaining comparisons.

While prefix truncation can be employed to all nodes in a B-tree, suffix
truncation pertains specifically to separator keys in branch nodes
{[}10{]}. Prefix truncation is most effective in leaf nodes whereas
suffix truncation primarily affects branch nodes and the root node. When
a leaf is split into two neighbor leaves, a new separator key is
required. Rather than taking the highest key from the left neighbor or
the lowest key from the right neighbor, the separator is chosen as the
shortest string that separates those two keys in the leaves.

For example, assume the key values shown in \autoref{fig-3-6} are in the middle
of a node that needs to be split. The precise center is near the long
arrow. The minimal key splitting the node there requires at least 9
letters, including the first letter of the given name. If, on the other
hand, a split point anywhere between the short arrows is acceptable, a
single letter suffices. A single comparison of the two keys defining the
range of acceptable split points can determine the shortest possible
separator key. For example, in \autoref{fig-3-6}, a comparison between
``Johnson, Lucy'' and ``Smith, Eric'' shows their first difference in
the first letter, indicating that a separator key with a single letter
suffices. Any letter can be chosen that is larger than J and not larger
than S. It is not required that the letter actually occurs in the
current key values.

\begin{figure}
  \centering
  \includegraphics[width=0.3\columnwidth]{./media/fig-3-6.png}

  \caption{Finding a separator key during a leaf split.\label{fig-3-6}}
\end{figure}

It is tempting to apply suffix truncation not only when splitting leaf
nodes but also when splitting branch nodes. The problem with this idea,
however, is that a separator key in a grandparent node must guide the
search not only to the correct parent but also to the correct leaf. In
other words, applying suffix truncation again might guide a search to
the highest node in the left sub-tree rather than to the lowest node in
the right sub-tree, or vice versa. Fortunately, if 99\% of all B-tree
nodes are leaves and 99\% of the remaining nodes are immediate parents
of leaves, additional truncation could benefit at most 1\% of 1\% of all
nodes. Thus, this problematic idea, even if it worked flawlessly, would
probably never have a substantial effect on B-tree size or search
performance.

\autoref{fig-3-7} illustrates the problem. The set of separator keys in the
upper B-tree is split by the shortened key ``g,'' but the set of leaf
entries is not. Thus, a root-to-leaf search for the key ``gh'' will be
guided to the right sub-tree and thus fail, obviously incorrectly. The
correct solution is to guide searches based on the original separator
key ``gp.'' In other words, when the branch node is split, no further
suffix truncation must be applied. The only choice when splitting a
branch node is the split point and the key found there.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{./media/fig-3-7.png}

  \caption{Incorrect suffix truncation.\label{fig-3-7}}
\end{figure}

\begin{itemize}
\item
  A simple technique for compression, particularly effective in leaf
  pages, is to identify the prefix shared by all key values and to store
  the prefix only once.
\item
  Alternatively, or in addition, binary search and interpolation search
  can ignore key bytes shared by the lower and upper bounds of the
  remaining search interval. In a root-to-leaf search, such dynamic
  prefix truncation carries from parent to child.
\item
  Key values in branch pages need not be actual key values. They merely
  need to guide root-to-leaf searching. When posting a separator key
  while splitting a leaf page, a good choice is the shortest value that
  splits near the middle.
\item
  Offset-value coding compares each key value with its immediate
  neighbor and truncates the shared prefix. It achieves better
  compression than page-wide prefix truncation but disables efficient
  binary search and interpolation search.
\item
  Normalized keys significantly reduce the implementation complexity of
  prefix and suffix truncation as well as of offsetvalue coding.
\end{itemize}

\hypertarget{cpu-caches}{%
\section{CPU Caches}\label{cpu-caches}}

Cache faults contribute a substantial fraction to the cost of searching
within a B-tree page. If a B-tree needs to be searched with many keys
and the sequence of search operations may be modified, temporal locality
may be exploited {[}128{]}. Otherwise, optimization of data structures
is required. Cache faults for instructions can be reduced by use of
normalized keys --- comparisons of individual fields with international
sort order, collation sequence, etc., plus interpretation of schema
information, can require a large amount of code whereas two normalized
keys can be compared by a single hardware instruction. Moreover,
normalized keys simplify the implementation not only of prefix and
suffix truncation but also of optimizations targeted at reducing cache
faults for data accesses. In fact, many optimizations seem practical
only if normalized keys are used.

After prefix truncation has been applied, many comparisons in a binary
search are decided by the first few bytes. Even where normalized keys
are not used in the records, e.g., in B-tree leaves, storing a few bytes
of the normalized key can speed up comparisons. If only those few bytes
are stored, not the entire normalized key, such that they can decide
many but not all comparisons, they are called ``poor man's normalized
keys'' {[}41{]}.

In order to enable key comparisons and search without cache faults for
data records, poor man's normalized keys can be an additional field in
the elements of the indirection vector. This design has been employed
successfully in the implementation of AlphaSort {[}101{]} and can be
equally beneficial in B-tree pages {[}87{]}.

On the other hand, it is desirable to keep each element in the
indirection vector small. While traditional designs often include the
record size in the elements of the indirection vector as was mentioned
in the discussion of \autoref{fig-3-3}, the record length is hardly ever
accessed without access to the related record. Thus, the field
indicating the record length might as well be placed with the main
record rather than in the indirection vector.

\autoref{fig-3-8} illustrates such a B-tree page with keys indicating three
European countries. On the left are page header and indirection vector,
on the right are the variable-size records. The poor man's normalized
key, indicated here by a single letter, is kept in the indirection
vector. The main record contains the total record size and the remaining
bytes of the key. A search for ``Denmark'' can eliminate all records by
the poor man's normalized keys without incurring cache faults for the
main records. A search for ``Finland,'' on the other hand, can rely on
the poor man's normalized key for the binary search but eventually must
access the main record for ``France.'' While the poor man's normalized
key in \autoref{fig-3-8} comprises only a single letter, 2 or 4 bytes seem more
appropriate, depending on the page size. For example, in a small
database page optimized for flash storage and its fast access latency, 2
bytes might be optimal; whereas in large database pages optimized for
traditional disks and their fast transfer bandwidth, 4 bytes might be
optimal.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{./media/fig-3-8.png}

  \caption{Poor man's normalized keys in the indirection vector.\label{fig-3-8}}
\end{figure}

An alternative design organizes the indirection vector not as a linear
array but as a B-tree of cache lines. The size of each node in this
B-tree is equal to a single cache line or a small number of them
{[}64{]}. Root-to-leaf navigation in this B-tree might employ pointers
or address calculations {[}110, 87{]}. Search time and cache faults
within a B-tree page may be cut in half compared to node formats not
optimized for CPU caches {[}24{]}. A complementary, more theoretical
design of cache efficient B-tree formats is even more complex but
achieves optimal asymptotic performance independently of the sizes of
disk page and cache line {[}11{]}. Both organizations of B-tree nodes,
i.e., linear arrays as shown in \autoref{fig-3-8} and B-trees within B-tree
nodes, can benefit from ghost slots, i.e., entries with valid key values
but marked invalid, which will be discussed shortly.

\begin{itemize}
  \item
A cache fault may waste 100s of CPU cycles. B-tree pages can be
optimized to reduce cache faults just like B-trees are optimized
(compared to binary trees) to reduce page faults.
\end{itemize}

\hypertarget{duplicate-key-values}{%
\section{Duplicate Key Values}\label{duplicate-key-values}}

Duplicate values in search keys are fairly common. Duplicate records are
less common but do occur in some databases, namely if there is confusion
between relation and table and if a primary key has not been defined.
For duplicate records, the standard representations are either multiple
copies or a single copy with a counter. The former method might seem
simpler to implement as the latter method requires maintenance of
counters during query operations, e.g., a multiplication in the
calculation of sums and averages, setting the counter to one during
duplicate elimination, and a multiplication of two counters in joins.

Duplicate key values in B-tree indexes are not desirable because they
may lead to ambiguities, for example during navigation from a secondary
index to a primary index or during deletion of B-tree entries pertaining
to a specific row in a table. Therefore, all B-tree entries must be made
unique as discussed earlier. Nonetheless, duplicate values in the
leading fields of a search key can be exploited to reduce storage space
as well as search effort.

The most obvious way to store non-unique keys and their associated
information combines each key value with an array representing the
information. In non-unique secondary indexes with record identifiers as
the information associated with a key, this is a traditional format. For
efficient search, for example during deletion, the list of record
identifiers is kept sorted. Some simple forms of compression might be
employed. One such scheme stores differences between neighboring values
using the minimal number of bytes instead of storing full record
identifiers. A similar scheme has been discussed above as an alternative
scheme for prefix B-trees. For efficient sequential search, offset-value
coding {[}28{]} can be adapted.

A more sophisticated variant of this scheme permits explicit control
over the key prefix stored only once and the record remainder stored in
an array. If, for example, the leading key fields are large with few
distinct values, and the final key field is small with very many
distinct values, then storing values of those leading fields once can
save storage space.

An alternative representation of non-unique secondary index employs
bitmaps. There are various forms and variants. These will be discussed
below.

The rows in \autoref{fig-3-9} show alternative representations of the same
information: (a) shows individual records repeating the duplicate key
value for each distinct record identifier associated with the key value,
which is a simple scheme that requires the most space. Example (b) shows
a list of record identifiers with each unique key value, and (c) shows a
combination of these two techniques suitable for breaking up extremely
long lists, e.g., those spanning multiple pages. Example (d) shows a
simple compression based on truncation of shared prefixes. For example,
``(9)2'' indicates that this entry is equal to the preceding one in its
first 9 letters or ``Smith, 471,'' followed by the string ``2.'' Note
that this is different from prefix B-trees, which truncate the same
prefix from all records in a page or B-tree nodes. Example (e) shows
another simple compression schemes based on run-length encoding. The
encoding ``4711(2)'' indicates a contiguous series with 2 entries
starting with 4711. Example (f) shows a bitmap as might be used in a
bitmap index. The leading value 4708 indicates the integer represented
by the first bit in the bitmap; the ``1'' bits in the bitmap represent
the values 4711, 4712, and 4723. Bitmaps themselves are often compressed
using some variant of run-length encoding. Without doubt, many readers
could design additional variations and combinations.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{./media/fig-3-9.png}

  \caption{Alternative representations of duplicates.\label{fig-3-9}}
\end{figure}

Each of these schemes has its own strengths and weaknesses. For example,
(d) seems to combine the simplicity of (a) with space efficiency
comparable to that of (b), but it might require special considerations
for efficient search, whether binary or interpolation search is
employed. In other words, there does not seem to be a perfect scheme.
Perhaps the reason is that compression techniques focus on sequential
access rather than random access within the compressed data structure.

These schemes can be extended for multi-column B-tree keys. For example,
each distinct value in the first field may be paired with a list of
values of the second field, and each of those has a list of detail
information. In a relational database about students and courses, as a
specific example, an index for a many-to-many relationship may have many
distinct values for the first foreign key (e.g., student identifier),
each with a list of values for the second foreign key (e.g., course
number), and additional attributes about the relationship between pair
of key values (e.g., the semester when the student took the course). For
information retrieval, a full-text index might have many distinct
keywords, each with a list of documents containing a given keyword, each
document entry having a list of occurrences of keywords with documents.
Ignoring compression, this is the basic format of many text indexes.

Duplicate key values pertain not only to representation choices but also
to integrity constraints in relational databases. B-trees are often used
to prevent violations of unique constraints by insertion of a duplicate
key value. Another technique, not commonly used, employs existing B-tree
indexes for instant creation and verification of newly defined
uniqueness constraints. During insertion of new key values, the search
for the appropriate insertion location could indicate the longest shared
prefix with either of the future neighbor keys. The required logic is
similar to the logic in dynamic prefix truncation. Based on the lengths
of such shared prefixes, the metadata of a B-tree index may include a
counter of distinct values. In multi-column B-trees, multiple counters
can be maintained. When a uniqueness constraint is declared, these
counters immediately indicate whether the candidate constraint is
already violated.

\begin{itemize}
\item
  Even if each B-tree entry is unique, keys might be divided into prefix
  and suffix such that there are many suffix values for each prefix
  value. This enables many compression techniques.
\item
  Long lists may need to be broken up into segments, with each segment
  smaller than a page.
\end{itemize}

\hypertarget{bitmap-indexes}{%
\section{Bitmap Indexes}\label{bitmap-indexes}}

The term bitmap index is commonly used, but it is quite ambiguous
without explanation of the index structure. Bitmaps can be used in
B-trees just as well as in hash indexes and other forms of indexes. As
seen in \autoref{fig-3-9}, bitmaps are one or many representation techniques
for a set of integers. Wherever a set of integers is associated with
each index key, the index can be a bitmap index. In the following,
however, a non-unique secondary B-tree index is assumed.

Bitmaps in database indexes are a fairly old idea {[}65, 103{]} that
gained importance with the rise of relational data warehousing. The only
requirement is a one-to-one mapping between information associated with
index keys and integers, i.e., the positions of bits in a bitmap. For
example, record identifiers consisting of device number, page number,
and slot number can be interpreted as a single large integer and thus
can be encoded in bitmaps and bitmap indexes.

In addition, bitmaps can be segmented and compressed. For segmentation,
the domain of possible bit positions is divided into ranges. These
ranges are numbered and a separate bitmap is created for each non-empty
range. The search key is repeated for each segment and extended by the
range number. An example for breaking lists into segments is shown in
\autoref{fig-3-9}, albeit with lists of references rather than with bitmaps.

A segment size with $2^15$ bit positions ensures that the
bitmap for any segment easily fits into a database page; a segment size
with $2^30$ bit positions ensures that standard integer
values can be used in compression by run-length encoding. Dividing
bitmaps into segments of $2^15$ or $2^30$
bit positions also enables reasonably efficient updates. For example,
insertion of a single record require decompression and re-compression of
only a single bitmap segment, and space management very similar to
changing the length of a traditional B-tree record.

For bitmap compression, most schemes rely primarily on run-length
encoding. For example, WHA {[}126{]} divides a bitmap into sections of
31 bits and replaces multiple neighboring sections with a count. In the
compressed image, a 32-bit word contains an indicator bit plus either a
literal bitmap of 31 bits or a run of constant values. In each run, a
30-bit count leaves one bit to indicate whether the replaced sections
contain ``0'' bits or ``1'' bits. Bitmap compression schemes based on
bytes rather than words tend to achieve tighter compression but require
more expensive operations {[}126{]}. This is true in particular if run
lengths are encoded in variable-length integers.

\autoref{fig-3-10} illustrates this compression technique. Example (a) shows a
bitmap similar to the ones in \autoref{fig-3-10} although with a different
third value. Example (b) shows WAH compression. Commas indicate word
boundaries in the compressed representation, underlined bit values
indicate the word usage. The bitmap starts with 151 groups of 31 ``0''
bits. The following two words show literal bitmaps; two are required
because bit positions 4711 and 4712 fall into different groups of 31 bit
positions. Five more groups of 31 ``0'' bits then skip forward toward
bit position 4923, which is shown as a single ``1'' bit in the final
literal group of 31 bits.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{./media/fig-3-10.png}

  \caption{A WAH-compressed bitmap.\label{fig-3-10}}
\end{figure}

Without compression, bitmap indexes are space-efficient only for very
few distinct key values in the index. With effective compression, the
size of bitmap indexes is about equal to that of traditional indexes
with lists of references broken into segments, as shown in \autoref{fig-3-10}.
For example, with WAH compression, each reference requires at most one
run of ``0'' sections plus a bitmap of 31 bits. A traditional
representation with record identifiers might also require 64 bits per
reference. Thus, bitmap indexes are useful for both sparse and dense
bitmaps, i.e., for both low- and high-cardinality attributes {[}125,
126{]}.

Bitmaps are used primarily for read-only or read-mostly data, not for
update-intensive databases and indexes. This is due to the perceived
difficulty of updating compressed bitmaps, e.g., insertion of a new
value in run-length encoding schemes such as WAH. On the other hand,
lists of record identifiers compressed using numeric differences are
very similar to the counters in run-length encoding. Update costs should
be very similar in these two compressed storage formats.

The primary operations on bitmaps are creation, intersection, union,
difference, and scanning. Bitmap creation occurs during index creation,
and, when bitmaps are used to represent intermediate query results,
during query execution. Bitmap intersection aids in conjunctive
(``and'') query predicates, union in disjunctive (``or'') predicates.
Note that range queries on integer keys can often be translated into
disjunctions, e.g., ``\ldots between 3 and 5'' is equivalent to
``$\ldots = 3$ or $\ldots = 4$ or $\ldots = 5$.'' Thus, even if
most query predicates are written as conjunctions rather than
disjunctions, union operations are important for bitmaps and lists of
references.

Using a bitmap representation for an intermediate query result
implicitly sorts the data. This is particularly useful when retrieving
an unpredictable number of rows from a table using references obtained
from a secondary index. Gathering references in a bitmap and than
fetching the required database rows in sorted order is often more
efficient then fetching the rows without sorting. A traditional sort
operation might require more memory and more effort than a bitmap.

In theory, bitmaps can be employed for any Boolean property. In other
words, a bit in a bitmap indicates whether or not a certain record has
the property of interest. The discussion above and the example in \autoref{fig-3-9}
implicitly assume that this property is equality with a certain key
value. Thus, there is a bitmap for each key value in an index indicating
the records with those key values. Another scheme is based on modulo
operations {[}112{]}. For example, if a column to be indexed is a 32-bit
integer, there are 32 bitmaps. The bitmap for bit position $k$
indicates the records in which the key value modulo
$2^k$ is nonzero. Queries need to perform
intersection and union operations. Many other schemes, e.g., based on
range predicates, could also be designed. O'Neil et al. {[}102{]} survey
many of the design choices.

Usually, bitmap indexes represent one-to-many relationships, e.g.,
between key values and references. In these cases, a specific bit
position is set to ``1'' in precisely one of the bitmaps in the index
(assuming there is a row corresponding to the bit position). In some
cases, however, a bitmap index may represent a many-to-many
relationship. In those cases, the same bit position may be set to ``1''
in multiple bitmaps. For example, if a table contains two foreign keys
to capture a many-to-many relationship, one of the foreign key columns
might provide the key values in a secondary index and the other foreign
key column is represented by bitmaps. As a more specific example, the
many-to-many relationship enrollment between students and courses might
be represented by a B-tree on student identifier. A student's courses
can be captured in a bitmap. The same bit position representing a
specific course is set to ``1'' in many bitmaps, namely in the bitmaps
of all students enrolled in that course.

\begin{itemize}
\item
  Bitmaps require a one-to-one relationship between values and bit
  positions.
\item
  Bitmaps and compressed bitmaps are just another format to represent
  duplicate (prefix) values.
\item
  Bitmaps can be useful to represent all suffix values associated with
  distinct prefix values.
\item
  Run-length encoding as a compression technique for bitmaps is similar
  to compressing a list of integer values by sorting the list and
  storing the differences between neighbors. Based on this similarity,
  both techniques for representing duplicate values can be similarly
  space efficient.
\end{itemize}

\hypertarget{data-compression}{%
\section{Data Compression}\label{data-compression}}

Data compression reduces the expense of purchasing storage devices. It
also reduces the cost to house, connect, power, and cool these devices.
Moreover, it can improve the effective scan bandwidth as well as the
bandwidths of utilities such as defragmentation, consistency checks,
backup, and restore. Flash devices, due to their high cost per unit of
storage space, are likely to increase the interest in data compression
for file systems, databases, etc.

B-trees are the primary data structure in databases, justifying
compression techniques tuned specifically for B-tree indexes.
Compression in B-tree indexes can be divided into compression of key
values, compression of node references (primarily child pointers), and
representation of duplicates. Duplicates have been discussed above; the
other two topics are surveyed here.

For key values, prefix and suffix truncation have already been
mentioned, as has single storage of non-unique key values. Compression
of normalized keys has also been mentioned, albeit as a problem without
published techniques. Another desirable form of compression is
truncation of zeroes and spaces, with careful attention to
order-preserving truncation in keys {[}2{]}.

Other order-preserving compression methods seem largely ignored in
database systems, for example order-preserving Huffman coding or
arithmetic coding. Order-preserving dictionary codes received initial
attention {[}127{]}. Their potential usage in sorting, in particular
sorting in database query processing, is surveyed elsewhere {[}46{]};
many of the considerations there also apply to B-tree indexes.

For both compression and de-compression, order-preserving Huffman codes
rely on binary trees. For static codes, the tree is similar to the tree
for nonorder-preserving techniques. Construction of a Huffman code
starts with each individual symbol forming a singleton set and then
repeatedly merges two sets of symbols. For a standard Huffman code, the
two sets with the lowest frequencies are merged. For an order-preserving
Huffman code, the pair of immediate neighbors with the lowest combined
frequency is chosen. Both techniques support static and adaptive codes.
Adaptive methods start with a tree created as for a static method but
modify it according to the actual, observed frequency of symbols in the
uncompressed stream. Each such modification rotates nodes in the binary
tree.

\autoref{fig-3-11}, copied from {[}46{]}, shows a rotation in the binary tree
central to encoding and decoding in order-preserving Huffman
compression. The leaf nodes represent symbols and the root-to-leaf paths
represent the encodings. With a left branch encoded by a 0 and a right
branch by a 1, the symbols ``A,'' ``B,'' and ``C'' have encodings ``0,''
``10,'' and ``11,'' respectively. The branch nodes of the tree contain
separator keys, very similar to separator keys in B-trees. The left tree
in \autoref{fig-3-11} is designed for relatively frequent ``A'' symbols. If the
symbol ``C'' is particularly frequent, the encoding tree can be rotated
into the right tree, such that the symbols ``A,'' ``B,'' and ``C'' have
encodings ``00,'' ``01,'' and ``1,'' respectively. The rotation from the
left tree in \autoref{fig-3-11} to the right tree is worthwhile if the
accumulated weight in leaf node C is higher than that in leaf node A,
i.e., if effective compression is more important for leaf node C than
for leaf node A. Note that the frequency of leaf node B is not relevant
and the size of its encoding is not affected by the rotation, and that
this tree transformation is not suitable to minimize the path to node B
or the representation of B.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{./media/fig-3-11.png}

  \caption{Tree rotation in adaptive order-preserving Huffman
  compression.\label{fig-3-11}}
\end{figure}

Compression of B-tree child pointers may exploit the fact that
neighboring nodes are likely to have been allocated in neighboring
locations while a B-tree is created from a sorted stream of future index
entries. In this case, child pointers in a parent page can be compressed
by storing not the absolute values of pointers but their numeric
differences, and by storing those in the fewest words possible
{[}131{]}. In the extreme case, a form of run-length encoding can be
employed that simply indicates a starting node location and the number
of neighbor nodes allocated contiguously. Since careful layout of B-tree
nodes can improve scan performance, such allocation of B-tree nodes is
often created and maintained using appropriate space management
techniques. Thus, this compression technique often applies and it is
used in products. In addition to child pointers within B-tree indexes, a
variant can also be applied to a list of references associated with a
key value in a non-unique secondary index.

Compression using numeric differences is also a mainstay technique in
document retrieval, where ``an inverted index... records, for each
distinct word or term, the list of documents that contain the term, and
depending on the query modalities that are being supported, may also
incorporate the frequencies and impacts of each term in each document,
plus a list of the positions in each document at which that word
appears. For effective compression, the lists of document and position
numbers are usually sorted and transformed to the corresponding sequence
of differences (or gaps) between adjacent values.'' {[}1{]}. Research
continues to optimize compression effectiveness, i.e., the bits required
for values and length indicators for the values, and decompression
bandwidth. For example, Anh and Moffat {[}1{]} evaluate schemes in which
a single length indicator applies to all differences encoded in a single
machine word. Many more ideas and techniques can be found in dedicated
books and surveys, e.g., {[}124, 129{]}.

\begin{itemize}
\item
  Various data compression schemes exist for separator keys and child
  pointers in branch nodes and for key values and their associated
  information in leaf nodes.
\item
  Standard techniques are truncation of blank spaces and zeroes,
  representing values by their difference from a base value, and
  representing a sorted list of numbers by their differences.
  Offset-value coding is particularly effective for sorted runs in a
  merge sort but can also be used in B-trees.
\item
  Order-preserving, dynamic variants exist for Huffman compression,
  dictionary compression, and arithmetic compression.
\end{itemize}

\hypertarget{space-management}{%
\section{Space Management}\label{space-management}}

It is sometimes said that, in contrast to heap files, B-trees have space
management for records built-in. On the other hand, one could also say
that record placement in B-trees offers no choice even if multiple pages
have some free space; instead, a new record must be placed where its key
belongs and cannot be placed anywhere else.

There are some opportunities for good space management, however. First,
when an insertion fails due to insufficient space in the appropriate
node, a choice is required among compaction (reclamation of free space
within the page), compression (re-coding keys and their associated
information), load balancing (among sibling nodes), and splitting. As
simple and local operations are preferable, the sequence given indicates
the best approach. Load balancing among two neighbors is rarely
implemented; load balancing among more than two neighbors hardly ever.
Some defragmentation utilities, however, might be invoked for specific
key ranges only rather than for an entire B-tree.

Second, when splitting and thus page allocation are required, the
location of the new page offers some opportunities for optimization. If
large range scans and index-order scans are frequent, and if the B-tree
is stored on disks with expensive seek operations, it is important to
allocate the new page near the existing page.

Third, during deletion, similar choices exist. Load balancing among two
neighbors can be required during deletion in order to avoid underflow,
whereas it is an optional optimization for insertion. A commonly used
alternative to the ``text book'' design for deletion in B-trees ignores
underflows and, in the extreme cases, permits even empty pages in a
B-tree. Space reclamation is left to future insertions or to a
defragmentation utility.

In order to avoid or at least delay node splits, many database systems
permit leaving some free space in every page during index creation, bulk
loading, and defragmentation. For example, leaving 10\% free space in
all branch nodes hardly affects their fan-out or the height of the tree,
but it reduces the overhead of node splits during transaction
processing. In addition, some systems permit leaving free pages on disk.
For example, if the unit of I/O in large scans contains multiple B-tree
nodes, it can be advantageous to leave a few pages unallocated in each
such unit. If a node splits, a nearby page is readily available for
allocation. Until many nodes in the B-tree have been split due to many
insertions, scan performance is not affected.

An interesting approach to free space management on disk relies on the
core logic of B-trees. O'Neil's SB-trees {[}104{]} allocate disk space
in large contiguous extents of many pages, leaving some free pages in
each extent during index creation and defragmentation. When a node
splits, a new node is allocated within the same extent. If that is not
possible because the entire extent is allocated, the extent is split
into two extents, each half full. This split is quite similar to a node
split in a B-tree. While simple and promising, this idea has not been
widely adopted. This pattern of ``self-similar'' data structures and
algorithms can be applied at multiple levels of the memory hierarchy.

\autoref{fig-3-12} shows the two kinds of nodes in an SB-tree. Both extents and
pages are nodes in the sense that they may overflow and then are split
in half. The child pointers in page 75.2 contain very similar values for
page identifiers and thus are amenable to compression. When, for
example, page 93.4 must be split in response to an insertion, the entire
extent 93 is split and multiple pages, e.g., 93.3--93.5, moved to a new
extent.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{./media/fig-3-12.png}

  \caption{Nodes in an SB-tree.\label{fig-3-12}}
\end{figure}

\begin{itemize}
\item
  B-trees rigidly place a new record according to its sort key but
  handle space management gracefully, e.g., by load balancing among
  neighbor nodes.
\item
  B-tree concepts apply not only to placement of records in pages but
  also to placement of pages in contiguous clusters of pages on the
  storage media.
\end{itemize}

\hypertarget{splitting-nodes}{%
\section{Splitting Nodes}\label{splitting-nodes}}

After a leaf node is split into two, a new separator key must be posted
in the parent node. This might cause an overflow in the parent,
whereupon the parent node must be split into two and a separator key
must be posted in the grandparent node. In the extreme case, nodes from
a leaf to the root must be split and a new root must be added to the
B-tree.

The original B-tree algorithms called for leaf-to-root splitting as just
described. If, however, multiple threads or transactions share a B-tree,
then the bottom-up (leaf-to-root) splits in one thread might conflict
with a top-down (root-to-leaf) search of the other thread. The earliest
design relied on the concept of a ``safe'' node, i.e., one with space
for on more insertion, and retained locks from the last safe node during
a root-to-leaf search {[}9{]}. A more drastic approach restricts each
B-tree to only one structural change at a time {[}93{]}. Three other,
less restrictive solutions have been used for this problem.

First, since only few insertions require split operations, one can force
such an insertion to perform an additional root-to-leaf traversal. The
first traversal determines the level at which a split is required. The
second traversal performs a node split at the appropriate level. If it
is unable to post the separator key as required, it stops and instead
invokes another root-to-leaf pass that performs a split at the next
higher level. This additional root-to-leaf traversal can be optimized.
For example, if the upper B-tree nodes have not been changed in the
meantime, there is no need to repeat binary search with known outcomes.

Second, the initial root-to-leaf search of an insertion operation may
verify that all visited nodes have sufficient free space for one more
separator key. A branch node without sufficient free space is split
preventively {[}99{]}. Thus, a single root-to-leaf search promises to
perform all insertions and node splits. If each node can hold hundreds
of separator keys, splitting a little earlier than truly required does
not materially affect B-tree space utilization, node fan-out, or tree
height.

Unfortunately, variable-length separator keys present a problem; either
the splitting decision must be extremely conservative or there may be
rare cases in which a second root-to-leaf pass is required as in the
first solution described in the preceding paragraph. In other words, an
implementation of the first solution might be required in any case. If
node splits are rare, adding a heuristic code path with its own test
cases, regression tests, etc. might not provide a worthwhile or even
measurable performance gain.

Third, splitting a B-tree node and posting a new separator key in the
node's parent is divided into two steps {[}81{]}. During the
intermediate state, which may last a long time but ideally does not, the
B-tree node looks similar to the ternary node in a 2-3-tree as shown in
\autoref{fig-2-2}. In other words, two separate steps split a full node in two
and post a separator key in the parent node. For a short time, the new
node is linked to the old neighbor, not its parent, giving rise to the
name B\textsuperscript{link}-trees. As soon as convenient, e.g., during
the next root-to-leaf traversal, the separator key and the pointer are
copied from the formerly overflowing sibling node to the parent node.

\begin{itemize}
  \item
  Some variations of the original B-tree structure enable high
concurrency and efficient concurrency control.
B\textsuperscript{link}-trees seem particularly promising although they
seem to have been overlooked in products.
\end{itemize}

\hypertarget{summary}{%
\section{Summary}\label{summary}}

In summary, the basic B-tree design, both data structure and algorithms,
have been refined in many ways in decades of research and implementation
efforts. Many industrial implementations employ many of the techniques
reviewed so far. Research that ignores or even contradicts these
techniques may be perceived as irrelevant to commercial database
management products.
